{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learnを用いたノートブック上での特徴量変換\n",
    "## 特徴量をSageMaker Feature Storeに保存\n",
    "\n",
    "このノートブックでは、生のテキストをBERT埋め込みに変換します。\n",
    "これにより、テキスト分類などの自然言語処理タスクを実行することができます。\n",
    "この特徴量をSageMaker Feature Storeに保存します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Mania!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BERT Mania](img/bert_mania.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT埋め込みを理解\n",
    "\n",
    "* Bidirectional Encoder Representations from Transformers [BERT](https://arxiv.org/abs/1810.04805)\n",
    "* Transformerアーキテクチャーの詳細については [Attention Is All You Need](https://arxiv.org/abs/1706.03762) を参照してください。\n",
    "\n",
    "<img src=\"img/bert_embeddings.png\" width=\"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bert_input_features.png\" width=\"80%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **input_ids**: \n",
    "事前学習済みBERTの語彙のID。トークンを表す。（トークンの数が `max_seq_length` より少ない場合は、0がパディングされる。）\n",
    "\n",
    "* **input_mask**: \n",
    "BERTが注目すべきトークンを（0または1で）指定する。input_id のパディング部分には、ベクトル要素のそれぞれに 0 を与える。\n",
    "\n",
    "* **segment_ids**:\n",
    "セグメントIDは、テキスト分類のような単一シーケンスのタスクでは常に 0 となる。質問回答や次文予測などの2シーケンスのタスクでは 1 も使用される。\n",
    "  \n",
    "* **label_id**: \n",
    "各トレーニングデータの行のラベル（star_rating 1〜5）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "import botocore.config\n",
    "\n",
    "config = botocore.config.Config(\n",
    "    user_agent_extra='dsoaws/1.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTの最大シーケンス長を定義\n",
    "\n",
    "ここでは、最大シーケンス長はレビューテキストの単語数分布に基づいて選択します。\n",
    "\n",
    "![](img/max_seq_length_viz.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging FaceとTensorFlowを用いて生テキストをBERT特徴量に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "REVIEW_BODY_COLUMN = \"review_body\"\n",
    "REVIEW_ID_COLUMN = \"review_id\"\n",
    "\n",
    "LABEL_COLUMN = \"star_rating\"\n",
    "LABEL_VALUES = [1, 2, 3, 4, 5]\n",
    "\n",
    "label_map = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    label_map[label] = i\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"BERT特徴量ベクトル\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, review_id, date, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.review_id = review_id\n",
    "        self.date = date\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class Input(object):\n",
    "    \"\"\"シーケンス分類で用いるトレーニング/テストの単一の入力\"\"\"\n",
    "\n",
    "    def __init__(self, text, review_id, date, label=None):\n",
    "        \"\"\"入力のコンストラクタ\n",
    "        Args:\n",
    "          text: 文字列。トークン化されていない一つ目のシーケンスのテキスト。\n",
    "            単一シーケンスのタスクではこのシーケンスのみを指定する。\n",
    "          label: (オプショナル) 文字列。サンプルのラベル。トレーニングや検証用のサンプルでは指定する。\n",
    "            テスト用のサンプルでは指定しない。\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.review_id = review_id\n",
    "        self.date = date\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "def convert_input(the_input, max_seq_length):\n",
    "    # まず、BERTが学習したデータ形式と合うようにデータを前処理する。\n",
    "    # 1. テキストを小文字にする（BERT lowercaseモデルを用いる場合）\n",
    "    # 2. トークン化する（例、\"sally says hi\" -> [\"sally\", \"says\", \"hi\"]）\n",
    "    # 3. 単語をWordPieceに分割（例、\"calling\" -> [\"call\", \"##ing\"]）\n",
    "    #\n",
    "    # この辺りの処理はTransformersライブラリのトークナイザーがまかなってくれます。\n",
    "\n",
    "    tokens = tokenizer.tokenize(the_input.text)\n",
    "    tokens.insert(0, '[CLS]')\n",
    "    tokens.append('[SEP]')\n",
    "    print(\"**{} tokens**\\n{}\\n\".format(len(tokens), tokens))\n",
    "\n",
    "    encode_plus_tokens = tokenizer.encode_plus(\n",
    "        the_input.text,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # 事前学習済みBERTの語彙ID。トークンを表す。（トークン数が `max_seq_length` 未満であれば0をパディングする）\n",
    "    input_ids = encode_plus_tokens[\"input_ids\"]\n",
    "\n",
    "    # BERTがどのトークンに注目するかを0/1で指定。`input_ids` のパディング部分のベクトル要素には0を割り当てる。\n",
    "    input_mask = encode_plus_tokens[\"attention_mask\"]\n",
    "\n",
    "    # テキスト分類のような単一シーケンスのタスクではセグメントIDは常に0とする。質問回答や次文予測のような2シーケンスタスクの場合は1を割り当てる。\n",
    "    segment_ids = [0] * max_seq_length\n",
    "\n",
    "    # それぞれのトレーニングデータの行のラベル（`star_rating` 1〜5）\n",
    "    label_id = label_map[the_input.label]\n",
    "\n",
    "    features = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id,\n",
    "        review_id=the_input.review_id,\n",
    "        date=the_input.date,\n",
    "        label=the_input.label,\n",
    "    )\n",
    "\n",
    "    print(\"**{} input_ids**\\n{}\\n\".format(len(features.input_ids), features.input_ids))\n",
    "    print(\"**{} input_mask**\\n{}\\n\".format(len(features.input_mask), features.input_mask))\n",
    "    print(\"**{} segment_ids**\\n{}\\n\".format(len(features.segment_ids), features.segment_ids))\n",
    "    print(\"**label_id**\\n{}\\n\".format(features.label_id))\n",
    "    print(\"**review_id**\\n{}\\n\".format(features.review_id))\n",
    "    print(\"**date**\\n{}\\n\".format(features.date))\n",
    "    print(\"**label**\\n{}\\n\".format(features.label))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def transform_inputs_to_tfrecord(inputs, output_file, max_seq_length):\n",
    "    # データをBERTが理解できるフォーマットに変換する\n",
    "    records = []\n",
    "    tf_record_writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (input_idx, the_input) in enumerate(inputs):\n",
    "        if input_idx % 10000 == 0:\n",
    "            print(\"Writing input {} of {}\\n\".format(input_idx, len(inputs)))\n",
    "\n",
    "        features = convert_input(the_input, max_seq_length)\n",
    "\n",
    "        all_features = collections.OrderedDict()\n",
    "\n",
    "        # input_ids、input_mask、segment_ids、label_idsを含んだTFRecordを作成\n",
    "        all_features[\"input_ids\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\n",
    "        all_features[\"input_mask\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\n",
    "        all_features[\"segment_ids\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\n",
    "        all_features[\"label_ids\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\n",
    "\n",
    "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\n",
    "        tf_record_writer.write(tf_record.SerializeToString())\n",
    "\n",
    "        # Feature Storeに格納する、すべての特徴量を含んだレコードを作成\n",
    "        records.append(\n",
    "            {\n",
    "                \"input_ids\": features.input_ids,\n",
    "                \"input_mask\": features.input_mask,\n",
    "                \"segment_ids\": features.segment_ids,\n",
    "                \"label_id\": features.label_id,\n",
    "                \"review_id\": the_input.review_id,\n",
    "                \"date\": the_input.date,\n",
    "                \"label\": features.label,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    tf_record_writer.close()\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTの処理の準備としての特徴量エンジニアリングのフェーズで、それぞれの生のレビュー文（`review_body`）から3つの特徴量ベクトルが作成されます。\n",
    "\n",
    "* **input_ids**: \n",
    "事前学習済みBERTの語彙のID。トークンを表す。（トークンの数が `max_seq_length` より少ない場合は、0がパディングされる。）\n",
    "\n",
    "* **input_mask**: \n",
    "BERTが注目すべきトークンを（0または1で）指定する。input_id のパディング部分には、ベクトル要素のそれぞれに 0 を与える。\n",
    "\n",
    "* **segment_ids**:\n",
    "セグメントIDは、テキスト分類のような単一シーケンスのタスクでは常に 0 となる。質問回答や次文予測などの2シーケンスのタスクでは 1 も使用される。\n",
    "\n",
    "また、それぞれの生のレビューデータから、1つのラベル（`star_rating`）が作成されます。\n",
    "\n",
    "* **label_id**: \n",
    "各トレーニングデータの行のラベル（star_rating 1〜5）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTに特化した特徴量エンジニアリングステップのデモ\n",
    "\n",
    "ここでは、少量のデータを使ってコードのデモを行っていますが、後ほど、強力なSageMakerクラスター上でより多くのデータにスケールします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Storeにはイベント時刻の特徴量が必要\n",
    "\n",
    "レコード識別子名とイベント時刻の特徴量の名前が必要です。\n",
    "これらを、データ内の対応する特徴量のカラムに対応させます。\n",
    "\n",
    "注: イベント時刻の型は、Fractional（秒単位のUnixタイムスタンプ）、または、String（ISO-8601フォーマット）のいずれかでなければなりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import strftime\n",
    "\n",
    "# timestamp = datetime.now().replace(microsecond=0).isoformat()\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    [\n",
    "        5,\n",
    "        \"ABCD12345\",\n",
    "        \"\"\"I needed an \"antivirus\" application and know the quality of Norton products.  This was a no brainer for me and I am glad it was so simple to get.\"\"\",\n",
    "    ],\n",
    "    [\n",
    "        3,\n",
    "        \"EFGH12345\",\n",
    "        \"\"\"The problem with ElephantDrive is that it requires the use of Java. Since Java is notorious for security problems I haveit removed from all of my computers. What files I do have stored are photos.\"\"\",\n",
    "    ],\n",
    "    [\n",
    "        1,\n",
    "        \"IJKL2345\",\n",
    "        \"\"\"Terrible, none of my codes worked, and I can't uninstall it.  I think this product IS malware and viruses\"\"\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"star_rating\", \"review_id\", \"review_body\"])\n",
    "\n",
    "# Input クラスを使用して、データからサンプルを作成する。\n",
    "inputs = df.apply(\n",
    "    lambda x: Input(label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date が Feature Store の仕様に合わせて ISO-8601 になっていることを確認\n",
    "print(inputs[0].date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord を保存\n",
    "\n",
    "3つの特徴ベクトルと1つのラベルは、`TFRecord` インスタンスのリストに変換されます（トレーニングデータの各行に1つずつ）。\n",
    "* **`tf_records`**:  トレーニングデータの各行のバイナリ表現（3つの特徴 + 1つのラベル）\n",
    "\n",
    "これらの `TFRecord` は、パイプラインの残りの部分で使用されるエンジニアリング済みの特徴量です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"./data-tfrecord-featurestore/data.tfrecord\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Feature Store に特徴量を追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeatureGroup を作成\n",
    "\n",
    "特徴量グループ（Feature Group）とは、Feature Store で定義される、特徴量を論理的にまとめたものです。特徴量グループの定義は、特徴量の定義のリスト、レコードの識別子名、オンラインストアとオフラインストアの設定などで構成されます。\n",
    "\n",
    "特徴量グループの管理には、create、describe、update、delete、list の各 API を使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "feature_group_name = \"reviews-feature-group-\" + strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "print(feature_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_definition import (\n",
    "    FeatureDefinition,\n",
    "    FeatureTypeEnum,\n",
    ")\n",
    "\n",
    "feature_definitions = [\n",
    "    FeatureDefinition(feature_name=\"input_ids\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"input_mask\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"segment_ids\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"label_id\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"review_id\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"date\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"label\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"split_type\", feature_type=FeatureTypeEnum.STRING),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "feature_group = FeatureGroup(name=feature_group_name, feature_definitions=feature_definitions, sagemaker_session=sess)\n",
    "print(feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `record identifier` と `event time` の特徴量を指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_feature_name = \"review_id\"\n",
    "event_time_feature_name = \"date\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## オフライン特徴量ストア用にS3プレフィックスをセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"reviews-feature-store-\" + timestamp\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Group を作成\n",
    "\n",
    "機能グループを作成するための最後のステップは、`create` メソッドを使用することです。\n",
    "オンラインストアはデフォルトでは作成されないので、有効にしたい場合は `enable_online_store=True` とする必要があります。\n",
    "`s3_uri`にはオフラインストアの場所を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group.create(\n",
    "    s3_uri=f\"s3://{bucket}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=role,\n",
    "    enable_online_store=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Group の詳細を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store に取り込むレコードをレビュー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = transform_inputs_to_tfrecord(inputs, output_file, max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _^^ ここ ^^の警告は無視してください_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Group の作成が完了するまでお待ちください\n",
    "\n",
    "## _注意:  完了まで数分かかります。しばらくお待ちください。_\n",
    "\n",
    "特徴量グループの作成は、データの読み込みに時間がかかります。\n",
    "作成されてからでないと使用できません。\n",
    "ステータスの確認は、以下の方法で行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_feature_group_creation_complete(feature_group=feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Store にレコードを取り込む\n",
    "\n",
    "FeatureGroup が作成されたら、`PutRecord` APIを使って FeatureGroup にデータを追加できるようになります。\n",
    "\n",
    "この API は高い TPS に対応でき、複数のストリームから呼び出せるように設計されています。\n",
    "それらの PUT リクエストからのデータはバッファリングされ、チャンクでS3に書き込まれます。\n",
    "\n",
    "ファイルは取り込まれてから数分以内にオフラインストアに書き込まれます。\n",
    "取り込み処理を高速化するために、複数のワーカーを指定して同時にジョブを実行させることもできます。\n",
    "\n",
    "FeatureGroup に単一のレコードを追加するには、`put_record(...)` を使います。\n",
    "\n",
    "pandas の DataFrame の内容を Feature Store に取り込むには、`ingest(...)` を使用してください。\n",
    "`max_worker` には、`data_frame` の複数のパーティションで並列に作業するために作成するスレッドの数を設定することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_records = pd.DataFrame.from_dict(records)\n",
    "df_records[\"split_type\"] = \"train\"\n",
    "df_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame の `Object` を、Feature Store でサポートされるデータ型の `String` にキャストする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_object_to_string(data_frame):\n",
    "    for label in data_frame.columns:\n",
    "        if data_frame.dtypes[label] == \"object\":\n",
    "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cast_object_to_string(df_records)\n",
    "\n",
    "feature_group.ingest(data_frame=df_records, max_workers=3, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量ストアがアクティブになるまで待機\n",
    "## _注意:  数分かかります。少々お待ちください。_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_describe_response = feature_group.describe()\n",
    "\n",
    "while \"OfflineStoreStatus\" not in feature_store_describe_response.keys():\n",
    "    feature_store_describe_response = feature_group.describe()\n",
    "    print(\"[INFO] Waiting for OfflineStore to be created.\")\n",
    "    # print(json.dumps(feature_store_describe_response, indent=4, sort_keys=True, default=str))\n",
    "    sleep(60)\n",
    "\n",
    "print(\"Offline store created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_store_status = None\n",
    "\n",
    "while offline_store_status != 'Active':\n",
    "    try:\n",
    "        offline_store_status = feature_group.describe()['OfflineStoreStatus']['Status']\n",
    "    except:\n",
    "        pass\n",
    "print('Offline store status: {}'.format(offline_store_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量ストアにクエリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_query = feature_group.athena_query()\n",
    "\n",
    "feature_store_table = feature_store_query.table_name\n",
    "\n",
    "query_string = \"\"\"\n",
    "    SELECT \n",
    "        input_ids,\n",
    "        input_mask,\n",
    "        segment_ids, \n",
    "        label_id,\n",
    "        review_id,\n",
    "        date,\n",
    "        label,\n",
    "        split_type\n",
    "    FROM \"{}\" \n",
    "    WHERE split_type='train' \n",
    "    LIMIT 3\n",
    "\"\"\".format(feature_store_table)\n",
    "\n",
    "print('Glue Catalog table name: {}'.format(feature_store_table))\n",
    "print('Running query: {}'.format(query_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_s3_uri = 's3://{}/query_results/{}/'.format(bucket, prefix)\n",
    "print(output_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_query.run(\n",
    "    query_string=query_string, \n",
    "    output_location=output_s3_uri\n",
    ")\n",
    "\n",
    "feature_store_query.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"max_colwidth\", 100)\n",
    "\n",
    "df_feature_store = feature_store_query.as_dataframe()\n",
    "df_feature_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量ストアをレビュー\n",
    "\n",
    "![Feature Store](img/feature_store_sm_extension.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# リソースを解放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
